{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ae287a-2765-409b-911b-729a67a0894a",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa2de1-8f33-42e7-ab02-2b90042f31bd",
   "metadata": {},
   "source": [
    "- 块由**类（class）**表示\n",
    "- 任何子类都必须定义一个将其输入转换为输出的**前向传播函数**，并且必须**储存任何必需的参数**\n",
    "- pytorch中自动微分，自动反向传播——**只需考虑前向传播函数和必需的参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f3c588-208b-438c-89ca-db5d45f54711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086899e-8bbe-4bac-a28e-f1bc3258a0b6",
   "metadata": {},
   "source": [
    "- `nn.Sequential`定义了一种**特殊**的`Module`\n",
    "- `net(X)`其实是`net.__call__(X)`的简写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834cf539-cbaa-4880-9007-a4c6a399665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0526, -0.0848, -0.1848, -0.0288,  0.0127, -0.1936,  0.2635,  0.0783,\n",
       "          0.0239,  0.1486],\n",
       "        [-0.1697, -0.0824, -0.1233,  0.0047, -0.0759, -0.1661,  0.3518,  0.1375,\n",
       "         -0.0747,  0.1867]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b42b9d-ef95-447a-9772-6cb777445077",
   "metadata": {},
   "source": [
    "### 自定义块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb99e2-129a-4ae0-bbf2-a41b72e5d72d",
   "metadata": {},
   "source": [
    "- 每卡UI必须提供的基本功能\n",
    "    1. 将输入数据作为其前向传播函数的参数\n",
    "    1. 通过前向传播函数来生成输出\n",
    "    1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问，通常是**自动发生的\n",
    "    1. 存储和访问前向传播计算所需的参数\n",
    "    1. 根据需要初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b476383e-9ffc-4de3-8b14-32201b5efcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1476, -0.0487, -0.0678,  0.0293, -0.0426, -0.0293,  0.0975, -0.0810,\n",
       "         -0.0974, -0.0766],\n",
       "        [ 0.1180, -0.1375, -0.0686,  0.0062, -0.0341, -0.0493,  0.1288, -0.1191,\n",
       "         -0.0283,  0.0184]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9c3d8-e743-4eaf-bc6f-a729065ab4ae",
   "metadata": {},
   "source": [
    "### 顺序块\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能\n",
    "- `*args`——可以传入多个神经网络层作为参数\n",
    "- `self._modules`是`nn.Module`内置的一个**有序字典**（OrderedDict） ，专门存储子模块\n",
    "    - `_modules`的**主要优点**是：在模块的**初始化**过程中，**系统知道**在`_modules`字典中查找需要初始化参数的子块\n",
    "    - `MySequential`的前向传播函数被调用时，每个添加的块都**按照被添加的顺序执行**\n",
    "    - `self._modules`**字典的键需要是*字符串***\n",
    "- `.values()`可以获取一个包含这些子模块的迭代器，可以遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38463ef4-1bad-4464-ba9c-746573282115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0788,  0.1800,  0.2064,  0.0026, -0.1285, -0.0084,  0.0960,  0.0186,\n",
       "         -0.1340,  0.0030],\n",
       "        [-0.1185,  0.1616,  0.0679, -0.0535, -0.1022, -0.0289,  0.1071,  0.0898,\n",
       "         -0.1266,  0.0189]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d10792-db31-4609-939f-ae5432692721",
   "metadata": {},
   "source": [
    "### 在前向传播函数中执行代码\n",
    "- 增加**常数参数**，不计算梯度`requires_grad=False`\n",
    "- 这里**必须是 `F.relu()` ！**，而不能是`nn.ReLU()`\n",
    "- 这里的权重`self.rand_weight`不是一个模型参数，故不会被反向传播更新\n",
    "- 这里的`while`控制流只是展示**如何将任意代码集成到神经网络计算的流程中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c354bd26-b607-4dbf-8502-0ab60df5a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数，在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53133da-05b7-4f7a-956f-7f762722da45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0113, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c674c8e-9a5b-421b-b4f4-2086cd6e7168",
   "metadata": {},
   "source": [
    "### 混合搭配各种组合块的方法\n",
    "- 下面只是为了展示如何灵活构造\n",
    "- `Sequential`的**输入可以是任何`nn.Module`的子类**\n",
    "- **层和块的顺序连接由`Sequential`块处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5375f7c9-84ed-4da8-8b5e-6d0d596d7e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0024, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f46ab-a57d-4dac-9f51-23a0179c452f",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的**顺序连接**由`Sequential`块处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa41cb-ab8b-4e2f-83a3-33047b7da857",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "    - 若将`MySequential`中储存块的方式从`OrderedDict`改为Python列表，代码可以正常运行\n",
    "    - 但无法像`_modules`一样使用`net.state_dict()`方便的访问**模型的网络结构和参数**，以字典的形式返回\n",
    "- `super(MySequential_list, self).__init__()`\n",
    "    - 显式指定了`super()`应该从`MySequential_list`类开始查找父类，并传入当前实例`self`\n",
    "    - 在复杂的多继承结构中，使用此方法可以**更直观**地控制从哪个类开始进行父类的查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc97eef2-fca5-4c34-bba3-c525af1e52f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3230,  0.3077, -0.5000, -0.0245, -0.1324, -0.3152,  0.4352, -0.1434,\n",
      "         -0.1767, -0.2672]], grad_fn=<AddmmBackward0>) \n",
      " tensor([[-0.0713,  0.0956, -0.2355,  0.0454, -0.0973, -0.2593, -0.0602, -0.2418,\n",
      "         -0.0377,  0.1806]], grad_fn=<AddmmBackward0>)\n",
      "MySequential_list() \n",
      " OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "class MySequential_list(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.sequential = []\n",
    "        for module in args:\n",
    "            self.sequential.append(module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.sequential:\n",
    "            X = module(X)\n",
    "        return X\n",
    "        \n",
    "X = torch.rand(1, 10)\n",
    "net = MySequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "net_list = MySequential_list(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "\n",
    "print(net(X), '\\n', net_list(X))\n",
    "# print(net, '\\n', net.state_dict())\n",
    "print(net_list, '\\n', net_list.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af804d-1936-46d8-be41-b9371c153470",
   "metadata": {},
   "source": [
    "2. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c9eb243-c797-418e-b379-d486003e44b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parallel(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "\n",
    "    def forward(self, X):\n",
    "        x1 = self.net1(X)\n",
    "        x2 = self.net2(X)\n",
    "        return torch.cat((x1, x2), dim=1)  # 横着连\n",
    "\n",
    "X = torch.rand(2, 10)\n",
    "net = Parallel(nn.Sequential(nn.Linear(10, 12), nn.ReLU()), \n",
    "               nn.Sequential(nn.Linear(10, 24), nn.ReLU()))\n",
    "output = net(X)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d874efde-be79-4f52-a6cd-2d7111358b0c",
   "metadata": {},
   "source": [
    "3. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n",
    "- 创建一个函数来实现：\n",
    "    - **生成多个块**：生成多个相同的层\n",
    "    - **连接这些块**：将生成的这些层按顺序连接起来，组成更大的网络\n",
    "- `.add_module(name, module)`是pytorch中`nn.Module`的方法，用于在现有网络中动态添加新的子模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bd7391a-56df-4b5e-b1c9-ede8e38ac911",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
       "  )\n",
       "  (out_put): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def creat_network(num_instances, input_size, hidden_size, output_size):\n",
    "    # 创建一个线性层\n",
    "    linear_layer = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "        nn.Linear(hidden_size, input_size)\n",
    "    )\n",
    "\n",
    "    # 创建多个实例并连接\n",
    "    instances = [linear_layer for _ in range(num_instances)] # 创建列表的简洁写法\n",
    "    network = nn.Sequential(*instances)                      # 解包运算符\n",
    "\n",
    "    # 添加输出层\n",
    "    output_layer = nn.Linear(input_size, output_size)\n",
    "    network.add_module('out_put', output_layer)\n",
    "\n",
    "    return network\n",
    "\n",
    "net = creat_network(3, 10, 5, 2)\n",
    "# net = creat_network(num_instances=3, input_size=10, hidden_size=5, output_size=2)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66de09db-a8be-4e42-974d-c1e8a0b2405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.activation(self.layer(X))\n",
    "\n",
    "def make_layers(input_size, output_size, num_blocks):\n",
    "    layers = []\n",
    "    for i in range(num_blocks):\n",
    "        layers.append(block(input_size, output_size))\n",
    "        input_size = output_size\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "num_blocks = 3 \n",
    "network = make_layers(10, 5, 3)\n",
    "\n",
    "x = torch.rand(2, 10) \n",
    "output = network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d2078-61c7-4b83-b6ce-cc16e6eb5582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
