{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28443207-9c10-4bc6-afb5-2ff9eb240517",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "- 访问参数，用于调试、诊断和可视化\n",
    "- 参数初始化\n",
    "- 在不同模型组件间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dee838-a61e-4fa3-9755-6971b2d230f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c595bd45-aab7-416d-a019-e07c8141cd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0046],\n",
       "        [-0.0713]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a59f2-286d-47a9-8e4d-e22448d2dd62",
   "metadata": {},
   "source": [
    "## 参数访问\n",
    "- 当通过`Sequential`类定义模型时，可以**通过索引**来访问模型的任意层\n",
    "- 参数是**复合的对象**，包含**值、梯度和额外信息**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5889c0d-65fd-4ecb-95a5-f51b0b8d10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.2660, -0.1183, -0.1681, -0.2617,  0.2256,  0.1269,  0.0813,  0.3273]])), ('bias', tensor([-0.1856]))])\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.1856], requires_grad=True)\n",
      "\n",
      "tensor([-0.1856])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())\n",
    "print()\n",
    "\n",
    "print(type(net[2].bias))\n",
    "print()\n",
    "print(net[2].bias)\n",
    "print()\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a3c41-4d63-4fc3-80c8-1b2c9454da30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None  # 还没有做反向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69cf26-408e-4c48-9b7a-408cc43ab748",
   "metadata": {},
   "source": [
    "## 一次性访问所有参数\n",
    "- `named_parameters()`：在模型的每一层中查找参数并返回一个**迭代器**，**生成`(name, parameter)`的二元组**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71a310e-2ea4-4e05-bcbb-e3439b99b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print()\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ece32b-3299-40a5-a3b1-3bff8c4f3528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1856])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec686a4-7170-4d52-ae3d-c53c330db8cd",
   "metadata": {},
   "source": [
    "## 从嵌套块收集参数\n",
    "- `.add_module(name, module)`是pytorch中`nn.Module`的方法，用于在现有网络中动态添加新的子模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "964649c0-16e5-406e-b9b4-88fa69d8765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3022],\n",
       "        [-0.3021]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb50ea2-1d98-48af-bd6e-c1a05ce6fa57",
   "metadata": {},
   "source": [
    "- 设计了网络后，看**如何工作的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43dfab4-3017-4ec4-8e5b-39b2f01b3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a93bc-3bb6-468f-99b5-2df61bc377d3",
   "metadata": {},
   "source": [
    "- 层是分层嵌套的，故也可以通过**嵌套列表索引一样**访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d80b69d8-ae60-4534-b527-62e5470ccc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1353, -0.4055, -0.1863, -0.4820, -0.4047,  0.1192,  0.2199,  0.3586])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df001b54-bfc6-4cbd-914e-4bad546bcaa8",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7b345-dff7-4bc4-8003-19187e36faff",
   "metadata": {},
   "source": [
    "### 内置初始化\n",
    "- `net.apply`：**递归地遍历并应用**自定义初始化或其他操作到**模型中的所有模块**的方法\n",
    "    - 可以在复杂的神经网络中，快速**对特定类型的层**进行操作\n",
    "- `net[0].weight.data[0]`取的是第一行的权重，包含该层第一个**输出节点**对所有输入节点的连接权重\n",
    "- `net[0].bias.data[0]`取的是**第一个输出节点**的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7a1451-9869-4d8c-a5e2-d0a28b578335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0069, -0.0144, -0.0026,  0.0213]), tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b43f1-0597-4eec-ae51-42167d51fb1f",
   "metadata": {},
   "source": [
    "- 从**API**角度来看，可以如下这么做——**但是！**不能这样做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3073c38-890d-4889-b60e-2721b987b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant_forbid(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant_forbid)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081669ca-ddcc-47ee-9766-5af49b275763",
   "metadata": {},
   "source": [
    "- 对某些块应用**不同**的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d38e48-8023-4b15-a348-82a44027315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3449,  0.6463,  0.4717, -0.5628])\n",
      "tensor([42., 42., 42., 42., 42., 42., 42., 42.])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346766f6-af46-4078-9dc7-8ad5f25bac1a",
   "metadata": {},
   "source": [
    "### 自定义初始化\n",
    "- 例子知识为了展示\n",
    "    - `m.named_parameters()][0]`中的`[0]`保证只获取**第一对**`(name, param_shape)`，线性层通常只包含`weight`和`bias`参数，`[0]`确保只输出第一个\n",
    "    - `nn.init.uniform`表示均匀分布\n",
    "    - 因为`*`解包的作用，`print`会**逐个打印出元组内的每个元素**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb85c4dc-926e-4424-aaa8-54b8cd2ca047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([8, 8])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-9.9931,  0.0000, -0.0000, -7.9628]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            'Init',\n",
    "            *[(name, param.shape) for name, param in m.named_parameters()][0]\n",
    "        )\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5  # 这个写法！\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight.data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3f487-5ef4-468b-9336-d626e474bb7d",
   "metadata": {},
   "source": [
    "更直接的方法——**直接设置**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745ac77d-41df-416d-9cc7-2bf843d15bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  0.5923,  0.9251,  0.9398])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1616b-da65-4278-a4d1-f094319cb030",
   "metadata": {},
   "source": [
    "## 参数绑定（共享）\n",
    "有时希望在多个层间**共享参数**：可以定义一个稠密层，然后使用其参数来设置另一层的参数\n",
    "- 第3和第5个神经网络层的参数是绑定的——不仅值相等，且**由相同的张量表示**（改名一个另一个也会**跟着改变**）\n",
    "- 在反向传播阶段，**梯度会互相影响**\n",
    "    - **梯度累加**：每层对损失的贡献都会计算一个梯度，因为共享相同的参数张量，给子计算的梯度会***累加*在同一张量上**\n",
    "    - **更新相同的参数**：统一更新共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc83c36-b58b-47cd-818b-f94786af779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), \n",
    "                    shared, nn.ReLU(), \n",
    "                    shared, nn.ReLU(), \n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == shared.weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5378a76-3fc4-4ceb-89ad-c215a6ae0d8a",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532effef-6e69-4e5e-84ef-1397258b12f5",
   "metadata": {},
   "source": [
    "## 练习\n",
    "1. 使用5.1节中定义的`NestMLP`模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c42beff-640a-4604-806e-61387aae155d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问net层的参数\n",
      "parameter name:0.weight, shape:torch.Size([64, 20]), parameter:Parameter containing:\n",
      "tensor([[-0.0511,  0.0768,  0.0607,  ...,  0.1370, -0.0483, -0.1949],\n",
      "        [ 0.0039, -0.0216, -0.1054,  ...,  0.1504,  0.0050, -0.2224],\n",
      "        [ 0.1375, -0.1107, -0.2031,  ...,  0.1635,  0.0671, -0.1086],\n",
      "        ...,\n",
      "        [-0.0564,  0.0200,  0.1294,  ...,  0.0462,  0.0958,  0.1335],\n",
      "        [-0.0062,  0.0008, -0.1809,  ..., -0.2223,  0.1046,  0.1954],\n",
      "        [ 0.0195, -0.1499, -0.2206,  ..., -0.1502,  0.0082, -0.2133]],\n",
      "       requires_grad=True)\n",
      "parameter name:0.bias, shape:torch.Size([64]), parameter:Parameter containing:\n",
      "tensor([-0.0896,  0.1172,  0.0579, -0.2124,  0.2234, -0.1427,  0.1706, -0.1779,\n",
      "        -0.0863, -0.0672,  0.1784, -0.1981, -0.1585, -0.2035, -0.2213,  0.0548,\n",
      "        -0.1894, -0.0575,  0.2096, -0.0115,  0.1678,  0.0135,  0.0088, -0.1482,\n",
      "         0.1532, -0.2022, -0.0273,  0.0917,  0.0760, -0.1379, -0.1597, -0.1613,\n",
      "         0.1149, -0.1224,  0.2055, -0.0873,  0.2128,  0.1726, -0.1427, -0.0819,\n",
      "         0.0832,  0.1315,  0.0672,  0.0409, -0.0925, -0.2014,  0.0895, -0.0604,\n",
      "        -0.1219, -0.0809,  0.0135,  0.0706, -0.0627, -0.0855,  0.0324, -0.0984,\n",
      "         0.2183,  0.0213, -0.1356, -0.1669, -0.0332,  0.2187,  0.0639, -0.0428],\n",
      "       requires_grad=True)\n",
      "parameter name:2.weight, shape:torch.Size([32, 64]), parameter:Parameter containing:\n",
      "tensor([[ 0.0768,  0.0566,  0.1017,  ...,  0.0544,  0.0633,  0.0662],\n",
      "        [-0.1131,  0.1121,  0.0086,  ...,  0.0666,  0.0929, -0.0731],\n",
      "        [-0.0446,  0.0773,  0.1075,  ..., -0.1183,  0.0147, -0.0373],\n",
      "        ...,\n",
      "        [ 0.0323, -0.1051,  0.0450,  ..., -0.1160, -0.1210,  0.0651],\n",
      "        [ 0.0040,  0.0100,  0.0908,  ..., -0.0880, -0.0964,  0.0718],\n",
      "        [ 0.1055,  0.0881, -0.0841,  ..., -0.0185, -0.0424, -0.0136]],\n",
      "       requires_grad=True)\n",
      "parameter name:2.bias, shape:torch.Size([32]), parameter:Parameter containing:\n",
      "tensor([ 0.1243,  0.0800, -0.0677,  0.0185,  0.0587, -0.0721,  0.0530, -0.0269,\n",
      "        -0.0941,  0.0333,  0.0640,  0.0863,  0.0409,  0.0240,  0.0991,  0.0984,\n",
      "        -0.0180, -0.0021, -0.0429, -0.0099,  0.1246,  0.0294,  0.0429, -0.0517,\n",
      "        -0.0015,  0.0788,  0.0396,  0.0383,  0.0613,  0.1168,  0.0155, -0.0778],\n",
      "       requires_grad=True)\n",
      "访问linear层的参数\n",
      "parameter name:weight, shape:torch.Size([16, 32]), parameter:Parameter containing:\n",
      "tensor([[ 1.1321e-01, -4.1714e-02, -8.6531e-02, -3.7693e-02, -1.2325e-01,\n",
      "         -1.7225e-01, -7.2031e-02,  2.4458e-02, -1.6376e-01,  1.3856e-01,\n",
      "         -1.0681e-01,  1.7422e-01,  1.4156e-02,  2.0473e-03,  1.6200e-01,\n",
      "          1.3875e-01, -1.2925e-01,  4.2320e-03, -5.4754e-02, -6.5671e-02,\n",
      "          5.8918e-02, -2.5665e-03,  1.4839e-01, -6.4630e-02,  1.0057e-02,\n",
      "          1.3971e-02, -4.0913e-02,  1.5952e-01, -5.9612e-02, -1.2829e-01,\n",
      "         -7.4926e-03, -2.8720e-02],\n",
      "        [ 9.9684e-02, -3.5216e-02,  1.6552e-01, -8.0440e-02, -6.8885e-04,\n",
      "          1.2562e-01,  8.0115e-02, -1.6759e-01, -6.2792e-02,  9.5362e-02,\n",
      "         -1.6351e-01,  9.4284e-02, -2.6887e-02,  1.5769e-01,  4.5786e-02,\n",
      "          3.2008e-02,  1.5828e-02,  1.5307e-01,  5.7015e-02,  8.2380e-02,\n",
      "         -1.6562e-01, -2.3247e-02,  1.5581e-01,  9.5956e-02, -7.1166e-02,\n",
      "         -1.0126e-01,  1.5080e-01, -3.1184e-02, -1.4138e-01, -1.3311e-01,\n",
      "         -7.4979e-02, -1.1535e-01],\n",
      "        [ 4.2936e-02,  1.6067e-01,  7.9396e-02,  1.2093e-01, -5.2132e-02,\n",
      "          9.4818e-02, -4.3061e-02, -1.6165e-01, -1.3337e-01, -1.6545e-01,\n",
      "          4.4977e-02,  1.5258e-02, -1.5388e-01, -1.1475e-01, -9.9640e-02,\n",
      "         -6.6956e-02, -1.6659e-01, -1.3119e-02, -4.2600e-02, -9.2529e-02,\n",
      "          1.4518e-02, -1.4505e-01, -6.0673e-02, -4.5574e-02,  5.9899e-02,\n",
      "          1.1004e-01,  1.6951e-01, -6.7605e-02,  3.7729e-02,  1.6315e-01,\n",
      "         -8.6551e-02,  9.5153e-02],\n",
      "        [-9.0729e-02, -4.8259e-02,  1.1901e-03, -7.7819e-02,  9.1986e-02,\n",
      "         -2.5569e-02,  7.4225e-02,  7.7306e-02,  1.0876e-03, -9.8090e-02,\n",
      "         -1.2483e-01, -9.3077e-02,  3.2694e-02,  3.6699e-02,  1.7369e-01,\n",
      "         -1.2675e-01,  3.7587e-02, -9.2824e-02, -1.3568e-01,  1.6746e-01,\n",
      "         -1.0497e-01, -7.5884e-02, -1.6977e-01,  1.6510e-01,  1.5801e-01,\n",
      "          1.1144e-01,  8.6653e-02,  1.7351e-01,  1.2564e-01,  1.6195e-01,\n",
      "          1.6758e-01, -5.6732e-02],\n",
      "        [ 4.3329e-02, -4.8333e-02, -5.9679e-02,  7.5864e-02,  8.1039e-02,\n",
      "         -1.5005e-01, -1.0794e-01, -3.8661e-02, -8.2054e-02, -1.6153e-01,\n",
      "         -4.2924e-02,  9.1767e-02,  1.2002e-01,  1.8609e-02,  7.9152e-02,\n",
      "          3.8673e-02, -1.1262e-01,  4.3433e-03, -5.8506e-03,  1.3332e-01,\n",
      "         -4.4879e-02,  1.3566e-01, -1.3045e-01, -3.9979e-02, -1.1494e-01,\n",
      "          7.1675e-02, -7.4904e-02,  1.2562e-02, -2.6783e-02, -9.6622e-02,\n",
      "          1.1388e-01,  3.6490e-02],\n",
      "        [-6.4467e-02,  9.4899e-02,  1.2368e-01,  8.8602e-02, -8.2974e-02,\n",
      "          1.5102e-01, -1.2430e-02,  1.2843e-01, -9.4358e-02,  1.2107e-01,\n",
      "         -1.4509e-01, -1.7104e-02,  9.8433e-02,  7.0965e-02,  1.4125e-01,\n",
      "         -1.1274e-01, -5.6444e-02,  1.5013e-01,  2.2163e-02,  1.5935e-01,\n",
      "         -2.8251e-02, -1.5321e-01, -8.0199e-02, -8.1674e-02, -1.6621e-01,\n",
      "         -4.5046e-02, -2.2401e-02, -1.4630e-01,  1.5957e-01, -3.6712e-02,\n",
      "         -1.2120e-01,  1.7480e-01],\n",
      "        [ 1.1273e-01,  3.4128e-02,  1.0924e-01,  1.4114e-01,  1.3554e-01,\n",
      "          1.6665e-01,  2.5874e-02, -3.5207e-02, -8.7926e-02, -1.3780e-01,\n",
      "          6.8435e-02,  1.4006e-01,  1.1443e-02, -6.4411e-02, -1.5142e-01,\n",
      "         -1.3518e-01, -5.3206e-02, -5.4560e-02, -1.4456e-01,  1.2393e-01,\n",
      "         -1.6253e-01,  2.5638e-02,  1.2520e-01, -6.7765e-02,  8.5512e-02,\n",
      "         -1.3706e-01,  2.3564e-02, -1.0215e-01, -1.1235e-01, -6.1709e-02,\n",
      "         -4.4775e-03,  7.8719e-02],\n",
      "        [ 9.1176e-02,  9.2045e-02,  1.1087e-01,  1.2680e-01, -4.2180e-02,\n",
      "          2.5247e-02,  1.1671e-01,  1.7075e-01, -1.4424e-01,  1.6790e-01,\n",
      "          2.4312e-02,  5.0658e-02,  1.4431e-01,  3.9200e-02, -1.4199e-01,\n",
      "         -1.2972e-01,  1.9842e-02,  2.0643e-03,  7.1270e-02,  4.6892e-02,\n",
      "         -1.5115e-01,  1.4770e-01,  7.4007e-02,  7.2971e-03,  7.6184e-03,\n",
      "          9.2429e-02, -1.7148e-01, -1.3733e-01,  1.1020e-01, -1.9837e-02,\n",
      "         -1.1964e-01, -1.1435e-01],\n",
      "        [ 8.6454e-02,  1.7501e-01, -3.5674e-02, -1.5024e-01, -3.9843e-02,\n",
      "         -1.0489e-01, -1.3703e-01,  1.1736e-01,  1.2429e-01,  1.7222e-01,\n",
      "          1.1610e-01,  1.6238e-01, -4.2838e-02,  9.9146e-02, -3.0463e-02,\n",
      "          9.4462e-02,  2.4686e-02, -1.2826e-01,  4.5418e-02,  3.0883e-02,\n",
      "          1.1243e-01, -1.4601e-02,  7.5091e-02, -7.4121e-02,  8.7522e-02,\n",
      "         -8.1143e-02,  1.2733e-01, -1.5610e-01,  7.9189e-03, -1.0831e-01,\n",
      "         -7.1267e-02,  1.4763e-01],\n",
      "        [ 1.5881e-01, -2.3727e-02, -2.2772e-02, -2.0227e-02, -1.3028e-01,\n",
      "         -7.0520e-02,  1.6260e-01,  6.8429e-02, -8.4858e-02, -9.0344e-02,\n",
      "         -1.9381e-02, -9.8701e-02,  1.0376e-01,  1.0817e-01, -4.6053e-02,\n",
      "         -4.9996e-02, -5.9990e-02,  4.9543e-02,  1.3175e-02, -1.3696e-01,\n",
      "         -4.0695e-02,  2.2990e-02,  5.4020e-02,  5.3042e-03, -1.9088e-02,\n",
      "         -1.6830e-01, -5.1646e-02,  7.8080e-02, -1.0578e-02, -2.0912e-02,\n",
      "          4.3195e-02,  1.0753e-01],\n",
      "        [-1.3583e-01, -2.8646e-02, -6.9874e-02,  6.5295e-02, -1.4261e-01,\n",
      "          1.3985e-01, -1.0177e-02, -9.5110e-02,  1.3176e-01, -7.7126e-02,\n",
      "          1.6723e-01,  1.7298e-01,  1.0695e-01,  1.0440e-01,  5.4381e-02,\n",
      "          1.3279e-02, -7.7772e-02, -1.1250e-01, -1.4890e-01,  7.4175e-02,\n",
      "         -1.7348e-01, -1.6591e-01,  4.6375e-02,  1.0114e-01,  1.6308e-01,\n",
      "          1.2508e-01,  1.7522e-01, -4.2589e-02,  1.3928e-01,  1.7182e-01,\n",
      "         -6.4092e-02, -6.9981e-02],\n",
      "        [ 1.7546e-01,  1.0118e-01,  1.7656e-01,  6.4467e-02,  1.0259e-01,\n",
      "         -1.4284e-01, -5.2471e-02,  1.0750e-01,  1.5054e-01, -1.0127e-01,\n",
      "          1.0248e-01,  1.6452e-01,  1.6201e-01,  1.0546e-01,  3.0632e-03,\n",
      "          2.9102e-02,  1.4984e-01, -1.1753e-01, -3.7960e-02, -7.8267e-02,\n",
      "          5.4712e-02,  1.3558e-01, -1.4808e-01, -8.2862e-02, -9.9936e-02,\n",
      "          1.4093e-01,  9.6430e-02,  4.5718e-02, -7.3357e-02,  7.4382e-02,\n",
      "         -1.3927e-01,  6.8875e-02],\n",
      "        [-1.1055e-01, -7.2057e-02,  1.0545e-01, -2.9695e-02, -5.1190e-02,\n",
      "         -7.3215e-02, -1.3524e-01, -1.4315e-01,  3.9310e-02, -3.4330e-02,\n",
      "         -7.9196e-02, -3.7315e-02,  3.0846e-03,  2.9185e-03,  1.5619e-01,\n",
      "          1.1092e-01,  3.4020e-02, -5.0075e-02,  7.3329e-02, -7.0911e-02,\n",
      "         -1.5323e-01,  8.1699e-02,  1.6151e-01, -7.5731e-02, -1.0011e-02,\n",
      "          8.9511e-02, -6.8417e-02,  1.0105e-01, -1.3919e-01,  7.3024e-02,\n",
      "          1.5189e-01, -6.6459e-02],\n",
      "        [-8.4594e-03, -4.8776e-02,  1.7618e-01, -1.5364e-01, -1.6894e-01,\n",
      "          4.8581e-02, -1.0885e-01, -1.3900e-01, -8.3261e-02,  1.9479e-02,\n",
      "         -1.6805e-01,  1.5325e-01, -8.9614e-02,  1.6054e-01, -1.4127e-01,\n",
      "         -4.3321e-02, -3.3636e-02, -9.3090e-02, -3.6661e-02,  1.3412e-01,\n",
      "         -1.2377e-01, -3.6059e-02,  7.5140e-02,  7.7964e-02,  2.7496e-02,\n",
      "         -1.2156e-01, -2.9658e-02,  5.0080e-02,  9.9963e-02,  1.6422e-01,\n",
      "          1.5901e-01,  8.5663e-02],\n",
      "        [-1.2452e-01,  2.0992e-02, -1.3870e-01,  1.6566e-04, -1.3314e-01,\n",
      "         -9.2711e-02, -4.6315e-02, -1.6426e-01, -1.1895e-01,  1.6750e-01,\n",
      "          2.3479e-03,  8.1386e-02,  2.7802e-02, -2.6911e-02,  9.6394e-03,\n",
      "         -4.4271e-02, -1.2987e-01,  1.1815e-01, -8.1035e-03,  1.5879e-01,\n",
      "         -5.4318e-02, -2.9299e-02, -1.2004e-02,  1.4041e-01, -5.3390e-02,\n",
      "         -1.6209e-01, -6.7507e-02,  2.8773e-02,  1.0868e-01,  6.0443e-02,\n",
      "         -9.5723e-02,  1.2271e-01],\n",
      "        [-1.1543e-01,  9.6366e-03, -1.7371e-01, -3.9358e-02, -2.4513e-02,\n",
      "         -6.2758e-02,  1.7581e-01,  8.5128e-02, -5.0107e-02, -6.8357e-03,\n",
      "         -1.9243e-02, -1.4239e-01,  3.2889e-02, -7.1301e-02,  1.0256e-01,\n",
      "         -8.4417e-02, -3.2105e-02,  5.2769e-02, -2.7789e-02, -1.4305e-01,\n",
      "          9.8310e-02, -1.0935e-02, -5.7419e-02, -1.6006e-02, -1.7051e-03,\n",
      "         -1.5233e-01,  1.1270e-01,  1.7495e-01, -1.7326e-01, -5.9236e-02,\n",
      "         -1.3606e-01, -1.3985e-01]], requires_grad=True)\n",
      "parameter name:bias, shape:torch.Size([16]), parameter:Parameter containing:\n",
      "tensor([ 0.1486,  0.1313, -0.0604, -0.0255,  0.0541, -0.1331,  0.0446,  0.0075,\n",
      "         0.1674,  0.0121,  0.1668,  0.1609,  0.0100,  0.1582, -0.1144,  0.1188],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "# print('访问各个层的参数')\n",
    "# net = NestMLP()\n",
    "# for name, param in net.named_parameters():\n",
    "#     print(f'parameter name:{name}, shape:{param.shape}, parameter:{param}')\n",
    "\n",
    "print('访问net层的参数')\n",
    "for name, param in net.net.named_parameters():\n",
    "    print(f'parameter name:{name}, shape:{param.shape}, parameter:{param}')\n",
    "\n",
    "print('访问linear层的参数')\n",
    "for name, param in net.linear.named_parameters():\n",
    "    print(f'parameter name:{name}, shape:{param.shape}, parameter:{param}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea37bf2-ada7-4239-b01d-b9b2e77b561a",
   "metadata": {},
   "source": [
    "2. 查看初始化模块文档以了解不同的初始化方法。\n",
    "- `torch.nn.init.uniform_(tensor, a=, b=)`\n",
    "- `torch.nn.init.normal_(tensor, mean=, std=)`\n",
    "- `torch.nn.init.constant_(tensor, val)`\n",
    "- `torch.nn.init.ones_(tensor)`\n",
    "- `torch.nn.init.zeros_(tensor)`\n",
    "- `torch.nn.init.xavier_uniform_(tensor, gain=)`\n",
    "- `torch.nn.init.xavier_normal(tensor, gain=)`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22d99b-46ec-4cb4-aece-cee4491270ee",
   "metadata": {},
   "source": [
    "3. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "- `shared_layer`层的参数和梯度都是**相同**的，因为共享同一个参数\n",
    "    - 损失函数`loss`也可以用`nn.functional.mse_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6ba0ce5-3f8b-4d4a-9cf2-955516266d20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \n",
      " tensor([[-0.2615, -0.3793, -0.0525, -0.4902],\n",
      "        [-0.0181, -0.3714, -0.4674, -0.4863],\n",
      "        [ 0.2014,  0.1035,  0.2618, -0.2640],\n",
      "        [-0.3097,  0.2032, -0.1983,  0.0953],\n",
      "        [-0.3038,  0.4684, -0.1896,  0.3220],\n",
      "        [-0.2873, -0.4536, -0.0364, -0.3834],\n",
      "        [ 0.3330, -0.0890,  0.1724, -0.4184],\n",
      "        [-0.1871,  0.0408,  0.0925, -0.2612]]) \n",
      " tensor([[-0.0350, -0.0252, -0.0342, -0.0042],\n",
      "        [ 0.0243,  0.0175,  0.0238,  0.0029],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0063, -0.0046, -0.0062, -0.0008],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0132,  0.0095,  0.0129,  0.0016],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000]])\n",
      "\n",
      "0.bias \n",
      " tensor([ 0.0979, -0.4704, -0.2744,  0.0315, -0.1829,  0.3130, -0.4065, -0.2642]) \n",
      " tensor([ 0.0226, -0.0157,  0.0000,  0.0041,  0.0000, -0.0085,  0.0000,  0.0000])\n",
      "\n",
      "2.weight \n",
      " tensor([[ 0.2578, -0.2951, -0.3418,  0.2458,  0.0512,  0.2972, -0.3084,  0.2374],\n",
      "        [ 0.2469, -0.2067,  0.1012, -0.1033, -0.2723, -0.2484,  0.3098, -0.2185],\n",
      "        [-0.2746, -0.2594, -0.0050,  0.2743, -0.1272,  0.1386, -0.1520, -0.3401],\n",
      "        [ 0.0769,  0.2487,  0.0849,  0.1859, -0.3380, -0.2997,  0.3396, -0.2835],\n",
      "        [-0.2097,  0.0938,  0.2891,  0.1983, -0.1553,  0.3379,  0.1515,  0.1410],\n",
      "        [-0.1900, -0.1615,  0.2859,  0.0245,  0.3409, -0.3426, -0.2532, -0.1651],\n",
      "        [ 0.0405, -0.1718, -0.0066,  0.0661,  0.1280,  0.1792,  0.1887, -0.0656],\n",
      "        [-0.0178, -0.2070, -0.0780,  0.0158,  0.0897, -0.0571, -0.2650, -0.1191]]) \n",
      " tensor([[ 0.0509,  0.0210,  0.0000,  0.0218,  0.0107,  0.0379,  0.0168,  0.0000],\n",
      "        [ 0.0123,  0.0000,  0.0000,  0.0037,  0.0062,  0.0000,  0.0098,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0970,  0.0196,  0.0000,  0.0353,  0.0351,  0.0353,  0.0553,  0.0000],\n",
      "        [-0.0598, -0.0420,  0.0000, -0.0309,  0.0000, -0.0757,  0.0000,  0.0000],\n",
      "        [-0.0136,  0.0000,  0.0000, -0.0041, -0.0069,  0.0000, -0.0109,  0.0000],\n",
      "        [ 0.0529,  0.0400,  0.0000,  0.0282, -0.0021,  0.0721, -0.0032,  0.0000],\n",
      "        [-0.0320,  0.0000,  0.0000, -0.0097, -0.0163,  0.0000, -0.0256,  0.0000]])\n",
      "\n",
      "2.bias \n",
      " tensor([ 0.0065, -0.0730, -0.0798,  0.2187, -0.1075,  0.2034,  0.2929,  0.1201]) \n",
      " tensor([ 0.0613,  0.0199,  0.0000,  0.1374, -0.0545, -0.0220,  0.0453, -0.0519])\n",
      "\n",
      "6.weight \n",
      " tensor([[-0.2114, -0.0892, -0.1978, -0.2857, -0.2813,  0.3324, -0.1261,  0.0755],\n",
      "        [-0.0156, -0.2501, -0.0086, -0.2622, -0.2071, -0.2310,  0.1279,  0.0550],\n",
      "        [-0.1404, -0.2038, -0.1034,  0.0019, -0.3170,  0.3505, -0.2015, -0.1096],\n",
      "        [-0.2609,  0.2606, -0.1116,  0.2130,  0.2607,  0.0716, -0.0714, -0.2571]]) \n",
      " tensor([[-0.0227, -0.0382, -0.0000, -0.1089, -0.0000, -0.0215, -0.1385, -0.0025],\n",
      "        [-0.0058, -0.0098, -0.0000, -0.0279, -0.0000, -0.0055, -0.0355, -0.0006],\n",
      "        [ 0.0125,  0.0209,  0.0000,  0.0597,  0.0000,  0.0118,  0.0760,  0.0014],\n",
      "        [ 0.0021,  0.0036,  0.0000,  0.0103,  0.0000,  0.0020,  0.0131,  0.0002]])\n",
      "\n",
      "6.bias \n",
      " tensor([-0.0249, -0.0908,  0.1982,  0.0682]) \n",
      " tensor([-0.2983, -0.0763,  0.1636,  0.0281])\n",
      "\n",
      "epoch:1, loss:0.12234896421432495\n",
      "\n",
      "0.weight \n",
      " tensor([[-0.2612, -0.3790, -0.0522, -0.4902],\n",
      "        [-0.0184, -0.3716, -0.4677, -0.4863],\n",
      "        [ 0.2014,  0.1035,  0.2618, -0.2640],\n",
      "        [-0.3096,  0.2032, -0.1982,  0.0953],\n",
      "        [-0.3038,  0.4684, -0.1896,  0.3220],\n",
      "        [-0.2874, -0.4537, -0.0365, -0.3834],\n",
      "        [ 0.3330, -0.0890,  0.1724, -0.4184],\n",
      "        [-0.1871,  0.0408,  0.0925, -0.2612]]) \n",
      " tensor([[-0.0340, -0.0245, -0.0333, -0.0041],\n",
      "        [ 0.0236,  0.0171,  0.0232,  0.0029],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0060, -0.0043, -0.0058, -0.0007],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0133,  0.0096,  0.0131,  0.0016],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000]])\n",
      "\n",
      "0.bias \n",
      " tensor([ 0.0976, -0.4702, -0.2744,  0.0315, -0.1829,  0.3131, -0.4065, -0.2642]) \n",
      " tensor([ 0.0220, -0.0153,  0.0000,  0.0039,  0.0000, -0.0086,  0.0000,  0.0000])\n",
      "\n",
      "2.weight \n",
      " tensor([[ 0.2573, -0.2953, -0.3418,  0.2456,  0.0511,  0.2968, -0.3086,  0.2374],\n",
      "        [ 0.2468, -0.2067,  0.1012, -0.1034, -0.2724, -0.2484,  0.3098, -0.2185],\n",
      "        [-0.2746, -0.2594, -0.0050,  0.2743, -0.1272,  0.1386, -0.1520, -0.3401],\n",
      "        [ 0.0759,  0.2485,  0.0849,  0.1856, -0.3384, -0.3000,  0.3391, -0.2835],\n",
      "        [-0.2091,  0.0942,  0.2891,  0.1986, -0.1553,  0.3386,  0.1515,  0.1410],\n",
      "        [-0.1899, -0.1615,  0.2859,  0.0245,  0.3410, -0.3426, -0.2531, -0.1651],\n",
      "        [ 0.0400, -0.1721, -0.0066,  0.0658,  0.1281,  0.1785,  0.1887, -0.0656],\n",
      "        [-0.0174, -0.2070, -0.0780,  0.0159,  0.0899, -0.0571, -0.2647, -0.1191]]) \n",
      " tensor([[ 0.0495,  0.0204,  0.0000,  0.0211,  0.0106,  0.0367,  0.0164,  0.0000],\n",
      "        [ 0.0116,  0.0000,  0.0000,  0.0035,  0.0060,  0.0000,  0.0093,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0948,  0.0192,  0.0000,  0.0343,  0.0349,  0.0346,  0.0540,  0.0000],\n",
      "        [-0.0586, -0.0412,  0.0000, -0.0302,  0.0000, -0.0743,  0.0000,  0.0000],\n",
      "        [-0.0131,  0.0000,  0.0000, -0.0039, -0.0068,  0.0000, -0.0105,  0.0000],\n",
      "        [ 0.0509,  0.0390,  0.0000,  0.0273, -0.0023,  0.0702, -0.0036,  0.0000],\n",
      "        [-0.0315,  0.0000,  0.0000, -0.0094, -0.0163,  0.0000, -0.0252,  0.0000]])\n",
      "\n",
      "2.bias \n",
      " tensor([ 0.0060, -0.0732, -0.0798,  0.2173, -0.1070,  0.2036,  0.2924,  0.1206]) \n",
      " tensor([ 0.0598,  0.0189,  0.0000,  0.1348, -0.0534, -0.0214,  0.0432, -0.0513])\n",
      "\n",
      "6.weight \n",
      " tensor([[-0.2112, -0.0889, -0.1978, -0.2846, -0.2813,  0.3326, -0.1247,  0.0756],\n",
      "        [-0.0156, -0.2500, -0.0086, -0.2620, -0.2071, -0.2309,  0.1282,  0.0550],\n",
      "        [-0.1405, -0.2040, -0.1034,  0.0013, -0.3170,  0.3504, -0.2022, -0.1096],\n",
      "        [-0.2609,  0.2606, -0.1116,  0.2129,  0.2607,  0.0716, -0.0715, -0.2571]]) \n",
      " tensor([[-0.0220, -0.0370, -0.0000, -0.1060, -0.0000, -0.0220, -0.1364, -0.0030],\n",
      "        [-0.0056, -0.0094, -0.0000, -0.0270, -0.0000, -0.0056, -0.0348, -0.0008],\n",
      "        [ 0.0122,  0.0205,  0.0000,  0.0588,  0.0000,  0.0122,  0.0756,  0.0017],\n",
      "        [ 0.0020,  0.0034,  0.0000,  0.0098,  0.0000,  0.0020,  0.0126,  0.0003]])\n",
      "\n",
      "6.bias \n",
      " tensor([-0.0219, -0.0901,  0.1966,  0.0679]) \n",
      " tensor([-0.2946, -0.0751,  0.1633,  0.0272])\n",
      "\n",
      "epoch:2, loss:0.11984328180551529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size, output_size = 4, 8, 4\n",
    "num_epochs, lr = 2, 0.01\n",
    "\n",
    "shared_layer = nn.Linear(hidden_size, hidden_size)\n",
    "MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "                    shared_layer, nn.ReLU(),\n",
    "                    shared_layer, nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, output_size)\n",
    "                   )\n",
    "\n",
    "X = torch.randn(1, input_size)\n",
    "y = torch.randn(1, output_size)\n",
    "\n",
    "trainer = torch.optim.SGD(MLP.parameters(), lr=lr)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    trainer.zero_grad()\n",
    "    l = loss(MLP(X), y)\n",
    "    l.backward()\n",
    "    trainer.step()\n",
    "    for name, param in MLP.named_parameters():\n",
    "        print(name,'\\n', param.data, '\\n', param.grad)\n",
    "        print()\n",
    "    print('epoch:{}, loss:{}'.format(epoch + 1, l.item()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a016f-4345-478a-80cf-92206e44f26f",
   "metadata": {},
   "source": [
    "4. 为什么共享参数是个好主意？\n",
    "- **节约内存**：共享参数可以减少模型中**需要存储的参数数量**，从而减少内存占用\n",
    "- **加速收敛**：共享参数可以让**模型更加稳定**，加速收敛\n",
    "- **提高泛化能力**：共享参数可以帮助模型更好**捕捉数据中的共性**，提高模型的泛化能力\n",
    "- **加强模型的可解释性**：共享参数可以让模型**更加简洁明了**，加强模型的可解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47250588-8a35-459a-868a-dcf3a7ba039f",
   "metadata": {},
   "source": [
    "# 延后初始化 \n",
    "defers initialization：直到数据第一次通过模型传递时，框架才会动态地推断出每层的大小\n",
    "- 在编写代码时无需知道维度是什么就可以设置参数——可以大大简化定义和修改模型的任务\n",
    "- **但是，pytorch中没有延后初始化**\n",
    "- 延后初始化**不是深度学习中的常见做法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25e0f0-ef0b-4851-8432-3ad342bc6c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
